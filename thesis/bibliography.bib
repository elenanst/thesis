@incollection{Turing:1995:CMI:216408.216410,
 author = {Turing, A. M.},
 chapter = {Computing Machinery and Intelligence},
 title = {Computers \&Amp; Thought},
 editor = {Feigenbaum, Edward A. and Feldman, Julian},
 year = {1995},
 isbn = {0-262-56092-5},
 pages = {11--35},
 numpages = {25},
 url = {http://dl.acm.org/citation.cfm?id=216408.216410},
 acmid = {216410},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@Article{Dolan2002,
author="Dolan, Elizabeth D.
and Mor{\'e}, Jorge J.",
title="Benchmarking optimization software with performance profiles",
journal="Mathematical Programming",
year="2002",
volume="91",
number="2",
pages="201--213",
abstract="We propose performance profiles --- distribution functions for a performance metric --- as a tool for benchmarking and comparing optimization software. We show that performance profiles combine the best features of other tools for performance evaluation.",
issn="1436-4646",
doi="10.1007/s101070100263",
url="http://dx.doi.org/10.1007/s101070100263"
}

@article{Demsar:2006:SCC:1248547.1248548,
 author = {Dem\v{s}ar, Janez},
 title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2006},
 volume = {7},
 month = dec,
 year = {2006},
 issn = {1532-4435},
 pages = {1--30},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=1248547.1248548},
 acmid = {1248548},
 publisher = {JMLR.org},
} 

@inproceedings{Bergstra:2011:AHO:2986459.2986743,
 author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal\'{a}zs},
 title = {Algorithms for Hyper-parameter Optimization},
 booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
 series = {NIPS'11},
 year = {2011},
 isbn = {978-1-61839-599-3},
 location = {Granada, Spain},
 pages = {2546--2554},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2986459.2986743},
 acmid = {2986743},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@Inbook{Hutter2011,
author="Hutter, Frank
and Hoos, Holger H.
and Leyton-Brown, Kevin",
editor="Coello, Carlos A. Coello",
title="Sequential Model-Based Optimization for General Algorithm Configuration",
bookTitle="Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="507--523",
isbn="978-3-642-25566-3",
doi="10.1007/978-3-642-25566-3_40",
url="http://dx.doi.org/10.1007/978-3-642-25566-3_40"
}

@ARTICLE{Neld:Wedd:1972,
  AUTHOR = {Nelder, J. A. and Wedderburn, R. W. M.},
  TITLE = {Generalized linear models},
  YEAR = {1972},
  JOURNAL = {Journal of the Royal Statistical Society, Series A, General},
  VOLUME = {135},
  PAGES = {370--384},
  KEYWORDS = {Probit analysis; Analysis of variance; Contingency table; Exponential family; Quantal response; Weighted least squares}
}

@article{Lambert:1992:ZPR:149268.149270,
 author = {Lambert, Diane},
 title = {Zero-inflated Poisson Regression, with an Application to Defects in Manufacturing},
 journal = {Technometrics},
 issue_date = {Feb. 1992},
 volume = {34},
 number = {1},
 month = feb,
 year = {1992},
 issn = {0040-1706},
 pages = {1--14},
 numpages = {14},
 url = {http://dx.doi.org/10.2307/1269547},
 doi = {10.2307/1269547},
 acmid = {149270},
 publisher = {American Society for Quality Control and American Statistical Association},
 address = {Alexandria, Va, USA},
} 

@ARTICLE{Famoye_onthe,
    author = {Felix Famoye and Karan P. Singh},
    title = {On the Generalized Poisson Regression Model with an application to Accident Data},
    journal = {Data Science},
    year = {},
    pages = {287--295}
}


@MastersThesis{gip,
    author     =     {Stewart, P.},
    title     =     {{A GENERALIZED INFLATED POISSON DISTRIBUTION}},
    school     =     {Marshall University},
    year     =     {2014},
    }


@Inbook{Hutter2011,
author="Hutter, Frank
and Hoos, Holger H.
and Leyton-Brown, Kevin",
editor="Coello, Carlos A. Coello",
title="Sequential Model-Based Optimization for General Algorithm Configuration",
bookTitle="Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="507--523",
isbn="978-3-642-25566-3",
doi="10.1007/978-3-642-25566-3_40",
url="http://dx.doi.org/10.1007/978-3-642-25566-3_40"
}

@inproceedings{Hutter:2009:EIM:1569901.1569940,
 author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin and Murphy, Kevin P.},
 title = {An Experimental Investigation of Model-based Parameter Optimisation: SPO and Beyond},
 booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '09},
 year = {2009},
 isbn = {978-1-60558-325-9},
 location = {Montreal, Qu\&\#233;bec, Canada},
 pages = {271--278},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1569901.1569940},
 doi = {10.1145/1569901.1569940},
 acmid = {1569940},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active learning, gaussian processes, noisy optimization, parameter tuning, sequential experimental design},
} 

@Online{wassenberg,
  hyphenation = {american},
  author      = {Fabian Pedregosa},
  title       = {Hyperparameter optimization with approximate gradient},
  version     = {1},
  date        = {2016-6-26},
  eprinttype  = {arxiv},
  eprintclass = {cs.LG},
  eprint      = {1602.02355v5}
}

@INPROCEEDINGS{1554741, 
author={S. A. Rojas and D. Fernan\-dez-Reyes}, 
booktitle={2005 IEEE Congress on Evolutionary Computation}, 
title={Adapting multiple kernel parameters for support vector machines using genetic algorithms}, 
year={2005}, 
volume={1}, 
pages={626-631 Vol.1}, 
keywords={genetic algorithms;learning (artificial intelligence);pattern classification;radial basis function networks;support vector machines;RBF kernel;SVM;classification tasks;genetic algorithm;genetic algorithms;grid-search;multiple kernel parameters;supervised learning problems;support vector machines;weighted radial basis function kernel;Algorithm design and analysis;Application software;Computer science;Educational institutions;Genetic algorithms;Grid computing;Kernel;Supervised learning;Support vector machine classification;Support vector machines}, 
doi={10.1109/CEC.2005.1554741}, 
ISSN={1089-778X}, 
month={Sept},}

@Online{ensemble,
  author      = {Lev{\'e}sque, Julien-Charles  and Gagn{\'e}, Christian and Sabourin, Robert},
  title       = {Bayesian Hyperparameter Optimization for Ensemble Learning},
  version     = {1},
  date        = {2016-5-20},
  eprinttype  = {arxiv},
  eprintclass = {cs.LG},
  eprint      = {1605.06394}
}

@inproceedings{Abdulrahman:2014:MCA:3015544.3015557,
 author = {Abdulrahman, Salisu Mamman and Brazdil, Pavel},
 title = {Measures for Combining Accuracy and Time for Meta-learning},
 booktitle = {Proceedings of the 2014 International Conference on Meta-learning and Algorithm Selection - Volume 1201},
 series = {MLAS'14},
 year = {2014},
 isbn = {1613-0073},
 location = {Prague, Czech Republic},
 pages = {49--50},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=3015544.3015557},
 acmid = {3015557},
 publisher = {CEUR-WS.org},
 address = {Aachen, Germany, Germany},
} 

@Online{practical,
  author      = {Snoek, Jasper and Larochelle, Hugo and Adams, P. Ryan},
  title       = {Practical Bayesian Optimization of Machine Learning Algorithms},
  version     = {2},
  date        = {2012-8-29},
  eprinttype  = {arxiv},
  eprintclass = {cs.LG},
  eprint      = {1206.2944}
}

@article{Bergstra:2012:RSH:2188385.2188395,
 author = {Bergstra, James and Bengio, Yoshua},
 title = {Random Search for Hyper-parameter Optimization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2012},
 volume = {13},
 month = feb,
 year = {2012},
 issn = {1532-4435},
 pages = {281--305},
 numpages = {25},
 url = {http://dl.acm.org/citation.cfm?id=2188385.2188395},
 acmid = {2188395},
 publisher = {JMLR.org},
 keywords = {deep learning, global optimization, model selection, neural networks, response surface modeling},
} 


@INPROCEEDINGS{7373431, 
author={M. Wistu\-ba and N. Schilling and L.\-Schmidt-Thieme}, 
booktitle={2015 IEEE International Conference on Data Mining}, 
title={Sequential Model-Free Hyperparameter Tuning}, 
year={2015}, 
pages={1033-1038}, 
keywords={pattern classification;support vector machines;AdaBoost;SMBO framework;SVM;acquisition function;automatic tuning;classifiers;complex hyperparameter tuning strategies;data sets;hyperparameter combinations;hyperparameter configurations;look-up tables;meta-features;random hyperparameter search;sequential model-based optimization;sequential model-free hyperparameter tuning;static ranking;surrogate models;Adaptation models;Data models;Loss measurement;Machine learning algorithms;Optimization;Tuning;hyperparameter optimization;meta-learning;transfer learning}, 
doi={10.1109/ICDM.2015.20}, 
ISSN={1550-4786}, 
month={Nov},}

@BOOK{Tukey,
  TITLE = {Exploratory Data Analysis},
  AUTHOR = {John W. Tukey},
  YEAR = {1977}, % I looked it up
  PUBLISHER = {Addison-Wesley},
}

@INPROCEEDINGS{Provost98onapplied,
    author = {Foster Provost and Ron Kohavi},
    title = {On Applied Research in Machine Learning},
    booktitle = {Machine learning},
    year = {1998},
    pages = {127--132}
}



@article{mann1947,
author = "Mann, H. B. and Whitney, D. R.",
doi = "10.1214/aoms/1177730491",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "03",
number = "1",
pages = "50--60",
publisher = "The Institute of Mathematical Statistics",
title = "On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other",
url = "http://dx.doi.org/10.1214/aoms/1177730491",
volume = "18",
year = "1947"
}












@article{10.2307/3001616,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/3001616},
 author = {William G. Cochran},
 journal = {Biometrics},
 number = {4},
 pages = {417-451},
 publisher = {[Wiley, International Biometric Society]},
 title = {Some Methods for Strengthening the Common χ<sup>2</sup> Tests},
 volume = {10},
 year = {1954}
}


@Article{McNemar1947,
author="McNemar, Quinn",
title="Note on the sampling error of the difference between correlated proportions or percentages",
journal="Psychometrika",
year="1947",
volume="12",
number="2",
pages="153--157",
abstract="Two formulas are presented for judging the significance of the difference between correlated proportions. The chi square equivalent of one of the developed formulas is pointed out.",
issn="1860-0980",
doi="10.1007/BF02295996",
url="http://dx.doi.org/10.1007/BF02295996"
}

@inproceedings{kuba2002exploiting,
  title={Exploiting sampling and meta-learning for parameter setting support vector machines},
  author={Kuba, Petr and Brazdil, Pavel and Soares, Carlos and Woznica, Adam},
  booktitle={Proceedings of the IBERAMIA},
  volume={2002},
  pages={217--225},
  year={2002}
}


@Article{Brazdil2003,
author="Brazdil, Pavel B.
and Soares, Carlos
and da Costa, Joaquim Pinto",
title="Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results",
journal="Machine Learning",
year="2003",
volume="50",
number="3",
pages="251--277",
abstract="We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.",
issn="1573-0565",
doi="10.1023/A:1021713901879",
url="http://dx.doi.org/10.1023/A:1021713901879"
}

@article{Wilcoxon45,
    author = {Wilcoxon, Frank},
    citeulike-article-id = {1712538},
    citeulike-linkout-0 = {http://dx.doi.org/10.2307/3001968},
    citeulike-linkout-1 = {http://www.jstor.org/stable/3001968},
    doi = {10.2307/3001968},
    issn = {00994987},
    journal = {Biometrics Bulletin},
    month = dec,
    number = {6},
    pages = {80--83},
    posted-at = {2008-06-08 18:56:28},
    priority = {2},
    publisher = {International Biometric Society},
    title = {{Individual Comparisons by Ranking Methods}},
    url = {http://dx.doi.org/10.2307/3001968},
    volume = {1},
    year = {1945}
}





@article{doi:10.1093/biomet/75.2.383,
author = {HOMMEL, G.},
title = {A stagewise rejective multiple test procedure based on a modified Bonferroni test},
journal = {Biometrika},
volume = {75},
pages = {383},
year = {1988},
doi = {10.1093/biomet/75.2.383},
URL = { + http://dx.doi.org/10.1093/biomet/75.2.383},
eprint = {/oup/backfile/Content_public/Journal/biomet/75/2/10.1093/biomet/75.2.383/2/75-2-383.pdf}
}






@Article{Mockus1991,
author="Mockus, J. B.
and Mockus, L. J.",
title="Bayesian approach to global optimization and application to multiobjective and constrained problems",
journal="Journal of Optimization Theory and Applications",
year="1991",
volume="70",
number="1",
pages="157--172",
abstract="In this paper, the Bayesian methods of global optimization are considered. They provide the minimal expected deviation from the global minimum. It is shown that, using the Bayesian methods, the asymptotic density of calculations of the objective function is much greater around the point of global minimum. The relation of this density to the parameters of the method and to the function is defined.",
issn="1573-2878",
doi="10.1007/BF00940509",
url="http://dx.doi.org/10.1007/BF00940509"
}

@Article{Kushner,
author="Kushner, H. J.
and Mockus, L. J.",
title="A New Method of Locating the Maximum Point of an Arbitrary Mult",
journal="Journal of Basic Engineering",
year="1964",
volume="86",
number="1",
pages="97--106",
issn="1573-2878",
doi="10.1115/1.3653121",
url="http://dx.doi.org/10.1115/1.3653121"
}


@Inbook{Brazdil1994,
author="Brazdil, Pavel
and Gama, Jo{\={a}}o
and Henery, Bob",
editor="Bergadano, Francesco
and De Raedt, Luc",
title="Characterizing the applicability of classification algorithms using meta-level learning",
bookTitle="Machine Learning: ECML-94: European Conference on Machine Learning Catania, Italy, April 6--8, 1994 Proceedings",
year="1994",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="83--102",
isbn="978-3-540-48365-6",
doi="10.1007/3-540-57868-4_52",
url="http://dx.doi.org/10.1007/3-540-57868-4_52"
}


@MISC{Reif_meta2-features:,
    author = {Matthias Reif and Faisal Shafait and Andreas Dengel},
    title = {Meta 2-Features: Providing Meta-Learners More Information},
    year = {}
}

@inproceedings{Bensusan:2001:EPA:645328.650030,
 author = {Bensusan, Hilan and Kalousis, Alexandros},
 title = {Estimating the Predictive Accuracy of a Classifier},
 booktitle = {Proceedings of the 12th European Conference on Machine Learning},
 series = {EMCL '01},
 year = {2001},
 isbn = {3-540-42536-5},
 pages = {25--36},
 numpages = {12},
 url = {http://dl.acm.org/citation.cfm?id=645328.650030},
 acmid = {650030},
 publisher = {Springer-Verlag},
 address = {London, UK, UK},
}


@incollection{craw1993,
title = "CONSULTANT: providing advice for the machine learning toolbox",
pages = "5-24",
doi = "10.1017/CBO9780511569944.002",
author = "S. Craw and D. Sleeman and N. Graner and M. Rissakis and S. Sharma",
abstract = "AbstractThe Machine Learning Toolbox (MLT), an Esprit project (P2154), provides an integrated toolbox of ten Machine Learning (ML) algorithms. One distinct component of the toolbox is Consultant, an advice-giving expert system, which assists a domain expert to choose and use a suitable algorithm for his learning problem. The University of Aberdeen has been responsible for the design and implementation of Consultant.Consultant's knowledge and domain is unusual in several respects. Its knowledge represents the integrated expertise of ten algorithm developers, whose algorithms offer a range of ML techniques; but also some algorithms use fairly similar approaches. The lack of an agreed ML terminology was the initial impetus for an extensive, associated help system. From an MLT user's point of view, an ML beginner requires significant assistance with terminology and techniques, and can benefit from having access to previous, successful applications of ML to similar problems; but in contrast" # " a more experienced user of ML does not wish constant supervision. This paper describes Consultant, discusses the methods used to achieve the required flexibility of use, and compares Consultant's similarities and distinguishing features with more standard expert system applications.INTRODUCTIONThe Machine Learning Toolbox (MLT), an Esprit project (P2154), provides an integrated toolbox of ten Machine Learning (ML) algorithms. One distinct component of the toolbox is Consultant, an advice-giving expert system. It provides domain experts with assistance and guidance on the selection and use of tools from the toolbox, but it is specifically aimed at experts who are not familiar with ML and its design has focused on their needs.",
url = "https://www.cambridge.org/core/books/research-and-development-in-expert-systems-ix/consultant-providing-advice-for-the-machine-learning-toolbox/26A8CCC0F5F5E4ECC6CB0B20E2C27083",
booktitle = "Research and Development in Expert Systems IX:",
publisher = "Cambridge University Press",
year = "1993",
month = "002",
day = "004",
address = "Cambridge"
}

@inproceedings{Kotsiantis:2007:SML:1566770.1566773,
 author = {Kotsiantis, S. B.},
 title = {Supervised Machine Learning: A Review of Classification Techniques},
 booktitle = {Proceedings of the 2007 Conference on Emerging Artificial Intelligence Applications in Computer Engineering: Real Word AI Systems with Applications in eHealth, HCI, Information Retrieval and Pervasive Technologies},
 year = {2007},
 isbn = {978-1-58603-780-2},
 pages = {3--24},
 numpages = {22},
 url = {http://dl.acm.org/citation.cfm?id=1566770.1566773},
 acmid = {1566773},
 publisher = {IOS Press},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Classifiers, Data Mining, Intelligent Data Analysis, Learning Algorithms},
} 

@inproceedings{Grubbs,
 author = {Grubbs, F. E},
 title = {Procedures for detecting outlying observations in samples },
 booktitle = {Technometrics, Volume 11 (1969) - Volume 1201},
 year = {1969},
 url = {http://www.citeulike.org/user/gatsoulis/article/7096000},
} 

@inproceedings{Caruana:2004:ESL:1015330.1015432,
 author = {Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
 title = {Ensemble Selection from Libraries of Models},
 booktitle = {Proceedings of the Twenty-first International Conference on Machine Learning},
 series = {ICML '04},
 year = {2004},
 isbn = {1-58113-838-5},
 location = {Banff, Alberta, Canada},
 pages = {18--},
 url = {http://doi.acm.org/10.1145/1015330.1015432},
 doi = {10.1145/1015330.1015432},
 acmid = {1015432},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@Inbook{Hutter2011,
author="Hutter, Frank
and Hoos, Holger H.
and Leyton-Brown, Kevin",
editor="Coello, Carlos A. Coello",
title="Sequential Model-Based Optimization for General Algorithm Configuration",
bookTitle="Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="507--523",
isbn="978-3-642-25566-3",
doi="10.1007/978-3-642-25566-3_40",
url="http://dx.doi.org/10.1007/978-3-642-25566-3_40"
}

@Inbook{Dietterich2000,
author="Dietterich, Thomas G.",
title="Ensemble Methods in Machine Learning",
bookTitle="Multiple Classifier Systems: First International Workshop, MCS 2000 Cagliari, Italy, June 21--23, 2000 Proceedings",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--15",
isbn="978-3-540-45014-6",
doi="10.1007/3-540-45014-9_1",
url="http://dx.doi.org/10.1007/3-540-45014-9_1"
}


@inproceedings{Feurer:2014:UMI:3015544.3015549,
 author = {Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
 title = {Using Meta-learning to Initialize Bayesian Optimization of Hyperparameters},
 booktitle = {Proceedings of the 2014 International Conference on Meta-learning and Algorithm Selection - Volume 1201},
 series = {MLAS'14},
 year = {2014},
 isbn = {1613-0073},
 location = {Prague, Czech Republic},
 pages = {3--10},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3015544.3015549},
 acmid = {3015549},
 publisher = {CEUR-WS.org},
 address = {Aachen, Germany, Germany},
} 



@article{DBLP:journals/corr/LoshchilovH16,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {{CMA-ES} for Hyperparameter Optimization of Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1604.07269},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.07269},
  timestamp = {Mon, 02 May 2016 18:22:52 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LoshchilovH16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@InProceedings{HutHooLeyMur10,
  author =	 {F. Hutter and H.~H. Hoos and K. Leyton-Brown and K.~P. Murphy},
  title =	 {Time-Bounded Sequential Parameter Optimization},
  booktitle = {Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 4)},
  year =	 2010,
  month =    jan,
}
@INPROCEEDINGS{1554761, 
author={T. Bartz-Beielstein and C. W. G. Lasarczyk and M. Preuss}, 
booktitle={2005 IEEE Congress on Evolutionary Computation}, 
title={Sequential parameter optimization}, 
year={2005}, 
volume={1}, 
pages={773-780 Vol.1}, 
keywords={heuristic programming;optimisation;search problems;statistical analysis;optimization algorithm;search algorithm;sequential parameter optimization;statistical techniques;Algorithm design and analysis;Computer science;Design for experiments;Design optimization;Evolutionary computation;Genetic algorithms;Modems;Performance analysis;Statistics;Stochastic processes}, 
doi={10.1109/CEC.2005.1554761}, 
ISSN={1089-778X}, 
month={Sept},}

@inproceedings{HutHooLey14:fanova,
	lauthor    = {Frank Hutter and Holger Hoos and Kevin Leyton-Brown},
	author    = {F. Hutter and H. Hoos and K. Leyton-Brown},
	title = {An Efficient Approach for Assessing Hyperparameter Importance},
	booktitle = {Proc.~of~ICML-14},
	year = {2014},
	note = {To appear}
}

@Inbook{Schilling2016,
author="Schilling, Nicolas
and Wistuba, Martin
and Schmidt-Thieme, Lars",
editor="Frasconi, Paolo
and Landwehr, Niels
and Manco, Giuseppe
and Vreeken, Jilles",
title="Scalable Hyperparameter Optimization with Products of Gaussian Process Experts",
bookTitle="Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="33--48",
isbn="978-3-319-46128-1",
doi="10.1007/978-3-319-46128-1_3",
url="http://dx.doi.org/10.1007/978-3-319-46128-1_3"
}

@paper{AAAI1510029,
	author = {Matthias Feurer and Jost Springenberg and Frank Hutter},
	title = {Initializing Bayesian Hyperparameter Optimization via Meta-Learning},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2015},
	keywords = {Machine Learning; Meta-Learning; Bayesian Optimization; Hyperparameter Optimization; Sequential Model-based Optimization},
	abstract = {Model selection and hyperparameter optimization is crucial in applying machine learning to a novel dataset. Recently, a subcommunity of machine learning has focused on solving this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for computationally expensive algorithms the overhead of hyperparameter optimization can still be prohibitive. In this paper we mimic a strategy human domain experts use: speed up optimization by starting from promising configurations that performed well on similar datasets. The resulting initialization technique integrates naturally into the generic SMBO framework and can be trivially applied to any SMBO method. To validate our approach, we perform extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with complementary strengths; optimizing two machine learning frameworks on 57 datasets. Our initialization procedure yields mild improvements for low-dimensional hyperparameter optimization and substantially improves the state of the art for the more complex combined algorithm selection and hyperparameter optimization problem.},

	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10029}
}

@Article{Huang2006,
author="Huang, D.
and Allen, T. T.
and Notz, W. I.
and Zeng, N.",
title="Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models",
journal="Journal of Global Optimization",
year="2006",
volume="34",
number="3",
pages="441--466",
abstract="This paper proposes a new method that extends the efficient global optimization to address stochastic black-box systems. The method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point. The criterion for the infill sample selection is an augmented expected improvement function with desirable properties for stochastic responses. The method is empirically compared with the revised simplex search, the simultaneous perturbation stochastic approximation, and the DIRECT methods using six test problems from the literature. An application case study on an inventory system is also documented. The results suggest that the proposed method has excellent consistency and efficiency in finding global optimal solutions, and is particularly useful for expensive systems.",
issn="1573-2916",
doi="10.1007/s10898-005-2454-3",
url="http://dx.doi.org/10.1007/s10898-005-2454-3"
}

@article{Nelder01011965,
author = {Nelder, J. A. and Mead, R.}, 
title = {A Simplex Method for Function Minimization},
volume = {7}, 
number = {4}, 
pages = {308-313}, 
year = {1965}, 
doi = {10.1093/comjnl/7.4.308}, 
abstract ={A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.}, 
URL = {http://comjnl.oxfordjournals.org/content/7/4/308.abstract}, 
eprint = {http://comjnl.oxfordjournals.org/content/7/4/308.full.pdf+html}, 
journal = {The Computer Journal} 
}

@article{DBLP:journals/corr/abs-1208-3719,
  author    = {Chris Thornton and
               Frank Hutter and
               Holger H. Hoos and
               Kevin Leyton{-}Brown},
  title     = {Auto-WEKA: Automated Selection and Hyper-Parameter Optimization of
               Classification Algorithms},
  journal   = {CoRR},
  volume    = {abs/1208.3719},
  year      = {2012},
  url       = {http://arxiv.org/abs/1208.3719},
  timestamp = {Wed, 10 Oct 2012 21:28:55 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1208-3719},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Bergstra:2011:AHO:2986459.2986743,
 author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal\'{a}zs},
 title = {Algorithms for Hyper-parameter Optimization},
 booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
 series = {NIPS'11},
 year = {2011},
 isbn = {978-1-61839-599-3},
 location = {Granada, Spain},
 pages = {2546--2554},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2986459.2986743},
 acmid = {2986743},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 



@INPROCEEDINGS{7373431, 
author={M. Wistu\-ba and N. Schilling and L.\-Schmidt-Thieme}, 
booktitle={2015 IEEE International Conference on Data Mining}, 
title={Sequential Model-Free Hyperparameter Tuning}, 
year={2015}, 
pages={1033-1038}, 
keywords={pattern classification;support vector machines;AdaBoost;SMBO framework;SVM;acquisition function;automatic tuning;classifiers;complex hyperparameter tuning strategies;data sets;hyperparameter combinations;hyperparameter configurations;look-up tables;meta-features;random hyperparameter search;sequential model-based optimization;sequential model-free hyperparameter tuning;static ranking;surrogate models;Adaptation models;Data models;Loss measurement;Machine learning algorithms;Optimization;Tuning;hyperparameter optimization;meta-learning;transfer learning}, 
doi={10.1109/ICDM.2015.20}, 
ISSN={1550-4786}, 
month={Nov},}

@Article{Soares2004,
author="Soares, Carlos
and Brazdil, Pavel B.
and Kuba, Petr",
title="A Meta-Learning Method to Select the Kernel Width in Support Vector Regression",
journal="Machine Learning",
year="2004",
volume="54",
number="3",
pages="195--209",
abstract="The Support Vector Machine algorithm is sensitive to the choice of parameter settings. If these are not set correctly, the algorithm may have a substandard performance. Suggesting a good setting is thus an important problem. We propose a meta-learning methodology for this purpose and exploit information about the past performance of different settings. The methodology is applied to set the width of the Gaussian kernel. We carry out an extensive empirical evaluation, including comparisons with other methods (fixed default ranking; selection based on cross-validation and a heuristic method commonly used to set the width of the SVM kernel). We show that our methodology can select settings with low error while providing significant savings in time. Further work should be carried out to see how the methodology could be adapted to different parameter setting tasks.",
issn="1573-0565",
doi="10.1023/B:MACH.0000015879.28004.9b",
url="http://dx.doi.org/10.1023/B:MACH.0000015879.28004.9b"
}



@paper{AAAI1510029,
	author = {Matthias Feurer and Jost Springenberg and Frank Hutter},
	title = {Initializing Bayesian Hyperparameter Optimization via Meta-Learning},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2015},
	keywords = {Machine Learning; Meta-Learning; Bayesian Optimization; Hyperparameter Optimization; Sequential Model-based Optimization},
	abstract = {Model selection and hyperparameter optimization is crucial in applying machine learning to a novel dataset. Recently, a subcommunity of machine learning has focused on solving this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for computationally expensive algorithms the overhead of hyperparameter optimization can still be prohibitive. In this paper we mimic a strategy human domain experts use: speed up optimization by starting from promising configurations that performed well on similar datasets. The resulting initialization technique integrates naturally into the generic SMBO framework and can be trivially applied to any SMBO method. To validate our approach, we perform extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with complementary strengths; optimizing two machine learning frameworks on 57 datasets. Our initialization procedure yields mild improvements for low-dimensional hyperparameter optimization and substantially improves the state of the art for the more complex combined algorithm selection and hyperparameter optimization problem.},

	url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10029}
}


@Inbook{Hutter2011,
author="Hutter, Frank
and Hoos, Holger H.
and Leyton-Brown, Kevin",
editor="Coello, Carlos A. Coello",
title="Sequential Model-Based Optimization for General Algorithm Configuration",
bookTitle="Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="507--523",
isbn="978-3-642-25566-3",
doi="10.1007/978-3-642-25566-3_40",
url="http://dx.doi.org/10.1007/978-3-642-25566-3_40"
}



@article{DBLP:journals/corr/LoshchilovH16,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {{CMA-ES} for Hyperparameter Optimization of Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1604.07269},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.07269},
  timestamp = {Mon, 02 May 2016 18:22:52 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LoshchilovH16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Introduction,
 author = {Presnell, Brett},
 title = {An introduction to Categorical Data Analysis Using R},
 issue_date = {28/1/2000}
}

@online{Fisher,
  author = {Wikipedia},
  title = {Fisher's exact test},
  year = 1999,
  url = {https://en.wikipedia.org/wiki/Fisher\%27s_exact_test},
  urldate = {2017-01-21}
}
@article{Introduction,
 author = {Presnell, Brett},
 title = {An introduction to Categorical Data Analysis Using R},
 issue_date = {28/1/2000}
}
@book{Mitchell:1997:ML:541177,
 author = {Mitchell, Thomas M.},
 title = {Machine Learning},
 year = {1997},
 isbn = {0070428077, 9780070428072},
 edition = {1},
 publisher = {McGraw-Hill, Inc.},
 address = {New York, NY, USA},
} 

@Inbook{Brazdil2009,
author = "Rostislav, Striz",
title="Extending Metalearning to Data Mining and KDD",
bookTitle="Metalearning: Applications to Data Mining",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="61--72",
isbn="978-3-540-73263-1",
doi="10.1007/978-3-540-73263-1_4",
url="http://dx.doi.org/10.1007/978-3-540-73263-1_4"
}


@article{10.2307/2984418,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984418},
 abstract = {In the analysis of data it is often assumed that observations y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>n</sub> are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
 author = {G. E. P. Box and D. R. Cox},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {211-252},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {An Analysis of Transformations},
 volume = {26},
 year = {1964}
}


%%% datasets %%%


