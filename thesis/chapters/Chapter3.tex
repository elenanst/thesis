\chapter{Περιγραφή Συστήματος}
Σε αυτό το κεφάλαιο θα περιγράψουμε το σύστημα Automated Data Scientist, έναν έμπειρο αυτοματοποιημένο αναλυτή δεδομένων για προβλήματα δυαδικής ταξινόμησης. Το λογισμικό είναι γραμμένο σε R και αποτελεί ένα command-line εργαλείο. Στη συνέχεια θα εξετάσουμε τη λειτουργικότητα, τις τεχνικές και την αρχιτεκτονική που επιστρατεύει προκειμένου να επιτελέσει το σκοπό του.
\section{Σκοπός}
Η εργασία μας ανταποκρίνεται στην ανάγκη της κοινότητας της μηχανικής μάθησης για εργαλεία AutoML, τα οποία συμβιβάζουν την αυτοματοποίηση με την κατανοησιμότητα, ώστε να υποβοηθούν, χωρίς να υποκαθιστούν τον αναλυτή. Συγκεκριμένα:
\begin{itemize}
	\item Στοχεύει στην επέκταση του state-of-the-art στη μετα-μάθηση εφαρμόζοντας μία τεχνική πρόβλεψης υπερ-παραμέτρων για τη ρύθμιση ενός μοντέλου.
	\item  Ενσωματώνει την εμπειρία της κοινότητας της μηχανικής μάθησης μέσω ευριστικών και κανόνων που έχουν προκύψει από άρθρα και τη γενικότερη βιβλιογραφία. Η λειτουργικότητα αυτή μιμείται τη προσέγγιση ενός αναλυτή δεδομένων, ο οποίος συχνά βασίζεται σε ευριστικές κατά τη λήψη σχεδιαστικών αποφάσεων.
	\item Εισάγει τη γλώσσα προγραμματισμού R στο σύνολο γλωσσών που χρησιμοποιούνται από εργαλεία AutoML, το οποίο απ' όσο γνωρίζουμε κυριαρχείται από τις γλώσσες python (SMAC, Spearmint, HPOlib, auto-sklearn, HyperOpt, TPO) και Java (AutoWEKA). Έτσι, ανοίγονται ευκαιρίες για εξερεύνηση και χρήση πακέτων της R κοινότητας, μιας δραστήριας και ετερογενούς ομάδας αναλυτών δεδομένων, μαθηματικών και προγραμματιστών.
	\item Αναγνωρίζει τη σημασία της διεπαφής μεταξύ χρήστη και συστήματος. Αν και το σύστημα θα είναι ανεξάρτητο χάρις στην εφαρμογή μετα-μάθησης και ευριστικών, είναι σημαντική η υποστήριξη της δυνατότητας επέμβασης του αναλυτή. Όσο αφορά την έξοδο του συστήματος θα εξασφαλίζεται η δυνατότητα επαναχρησιμοποίησης του παραγόμενου μοντέλου και η  κατανοητή παρουσίαση χρήσιμης γνώσης που παράχθηκε στη διάρκεια του πειράματος. 
\end{itemize} 
\section{Τεχνικές} \label{sec:techniques}
Προκειμένου να ικανοποιήσει το στόχο του το σύστημα χρησιμοποιεί διάφορες τεχνικές εφαρμογής μηχανικής μάθησης, εμπνευσμένες από τη βιβλιογραφία και προσαρμοσμένες στις ανάγκες του. Ένα εργαλείο \gls{AutoML} θα μπορούσε να αποτελείται εξολοκλήρου από έναν άκριτο συγκερασμό μεθόδων, μια τέτοια προσέγγιση ωστόσο θα ήταν υπολογιστικά και χρονικά ασύμφορη και, το σημαντικότερο, δε θα παρήγαγε γνώση χρήσιμη για την επιστήμη της μηχανικής μάθησης. Έχοντας αναγνωρίσει (Ενότητα 1.1) αυτήν την αντιμετώπιση ως κύριο αίτιο των παθογενειών της μηχανικής μάθησης, σχεδιάσαμε το σύστημα έτσι ώστε να ενσωματώνει τεχνικές του \gls{AutoML}, οι οποίες του εξασφαλίζουν αποδοτικότητα και το καθιστούν εκπαιδευόμενο, έμπειρο και επεκτάσιμο. Στη συνέχεια θα αναλύσουμε τις τεχνικές αυτές ως προς τη συνεισφορά και την υλοποίησή τους.
\subsection{Βελτιστοποίηση υπερ-παραμέτρων με μετα-μάθηση και χρήση διαστημάτων πρόβλεψης} \label{sec:HPP}
Η τεχνική αυτή αφορά το στάδιο της εκπαίδευσης ενός μοντέλου, συγκεκριμένα τη ρύθμισή του. Η επιλογή μας να υποκαταστήσουμε την αναζήτηση των υπερ-παραμέτρων με πρόβλεψή τους προσφέρει τα εξής οφέλη:
\begin{itemize}
	\item Υπολογιστική και χρονική βελτίωση. Το πρόβλημα ης βελτιστοποίησης, όπως έχουμε δει στην ενότητα \ref{section:opt}, απαιτεί μία επαναληπτική διαδικασία αξιολόγησης μίας κοστοβόρας συνάρτησης που ενέχει το κίνδυνο προσκόλλησης σε τοπικά μέγιστα, αποτελώντας το κατεξοχήν χρονοβόρο στάδιο ενός πειράματος. Αντιθέτως η πρόβλεψη των τιμών με χρήση μοντέλων μηχανικής μάθησης είναι χρονικά σύντομη και υπολογιστικά απλή.
	\item Ακόμη σημαντικότερο είναι το όφελος της εκπαιδευσιμότητας της μεθόδου μας. Η χρήση του συστήματός μας σε νέα προβλήματα το καθιστά πιο έμπειρο, γεγονός που αποτελεί μελλοντική επένδυση στη προσπάθειά του. Σε αντίθεση η αναζήτηση ξεκινά από μηδενική βάση σε κάθε πρόβλημα, καθώς δεν ενσωματώνει μετα-μάθηση. 
\end{itemize}

Προκειμένου να εκπαιδεύσουμε το μοντέλο θα χρειαστεί να συλλέξουμε επαρκή σετ δεδομένων, για τα οποία θα βελτιστοποιήσουμε τις υπερ-παραμέτρους και θα υπολογίσουμε τα μετα-χαρακτηριστικά. Η αναζήτηση των βέλτιστων υπερ-παραμέτρων έγινε με χρήση του αλγορίθμου TPE της βιβλιοθήκης HPOlib και η εξαγωγή των μετα-χαρακτηριστικών βασίστηκε στη δουλειά της ομάδας auto-sklearn (Ενότητα \ref{section:tools}). Στο σχήμα \ref{fig: flowHPP} βλέπουμε τη διαδικασία εκπαίδευσης του μοντέλου πρόβλεψης υπερ-παραμέτρων, το οποίο στο εξής θα αποκαλούμε HPP (Hyperparameter Prediction) μοντέλο.

Όσο αφορά τις υπερ-παραμέτρους διακρίνουμε 3 είδη, τα οποία απαιτούν διαφορετική αντιμετώπιση:
\begin{itemize}
	\item συνεχείς τιμές, για παράδειγμα η παράμετρος που ορίζει το πλάτος της συνάρτησης ακτινικής βάσης ενός SVM με χρήση γκαουσιανού πυρήνα ή η παράμετρος επιβολής βαρών κανονικοποίησης σε ένα \gls{ΤΝΔ}. Για τη πρόβλεψή τους απαιτείται η εκπαίδευση ενός μοντέλου παλινδρόμησης.
	\item ακέραιες τιμές, όπως το πλήθος των γειτόνων στον αλγόριθμο k-κοντινότερου γείτονα ή το βάθος ενός \gls{ΤΝΔ}. Εδώ εκπαιδεύεται επίσης ένα μοντέλο παλινδρόμησης και στη συνέχεια επιλέγεται η πλησιέστερη ακέραια τιμή.
	\item κατηγορικές τιμές, όπως η χρήση πυρήνα σε ένα bayesian μοντέλο. Στην προκειμένη απαιτείται η εκπαίδευση ενός μοντέλου ταξινόμησης.
\end{itemize}


\begin{figure}[!htb]
 	\begin{tikzpicture}[node distance = 3cm]
 	% Place nodes
 	\node [block_flow] (gather) {\makecell{Συλλογή \\σετ \\δεδομένων}};
 	\node [block_flow, right of=gather] (clean) {Καθαρισμός};
 	\node [block_flow, right of=clean, yshift = 1cm] (optimize) {Βελτιστοποίηση};
 	\node [block_flow, align = center, right of=clean, yshift = -1cm] (meta) {\makecell{Εξαγωγή \\ μετα-\\χαρακτηριστικών}};
 	\node [decision, right of=optimize, yshift = -1cm, scale = 0.8] (decide) {OK?};
 	\node [block_flow, right of=decide] (train) {Εκπαίδευση HPP};
 	% Draw edges
   \path [line] (gather) -- (clean);
   \path [line] (clean) |- (optimize.west);
   \path [line] (clean) |- (meta.west);
   \path [line] (optimize) -| (decide.north);
   \path [line] (meta) -| (decide.south);
   \path [line] (decide) --  node [anchor=south] {ΝΑΙ} (train);
   \path[line] (decide.south)  node [anchor=west] {OXI} |- ([xshift=-0.5cm,yshift=-0.5cm]meta.south west) -|  ($(gather)!.5!(clean)$);    
 	\end{tikzpicture}
 	\caption[Διάγραμμα ροής της διαδικασίας εκπαίδευσης του HPP μοντέλου]{Διάγραμμα ροής της διαδικασίας εκπαίδευσης του HPP μοντέλου: αρχικά συλλέγονται τα σετ δεδομένων και στη συνέχεια για το καθένα γίνεται εξαγωγή μετα-χαρακτηριστικών και βελτιστοποίηση υπερ-παραμέτρων. Η συνθήκη τερματισμού ελέγχει αν έχει ολοκληρωθεί η διαδικασία για όλα τα σετ δεδομένων. Τέλος, εκπαιδεύεται το μοντέλο, για το οποίο παράγεται επίσης πληροφορία για τα διαστήματα πρόβλεψης.}
 	\label{fig: flowHPP}	
\end{figure}
 \paragraph{Εκμετάλλευση διαστημάτων πρόβλεψης} Η απαίτηση ακριβούς πρόβλεψης της βέλτιστης τιμής μιας υπερ-παραμέτρου κρίνεται, τουλάχιστον με τα τρέχοντα μετα\-χαρακτηριστικά, υπερβολικά απαιτητική, διαπίστωση που οδηγεί στην απαίτηση τεχνικών αποδεοτικότερης εκμετάλλευση των μετα\-μοντέλων. Οι \citet{Feurer:2014:UMI:3015544.3015549} αρκέστηκαν στη χρήση των προβλέψεών τους για την εκκίνηση αλγορίθμων βελτιστοποίησης, ενώ οι \citet{Soares2004} πρότειναν την κατάταξη προκαθορισμένων συνδυασμών υπερ\-παραμέτρων έναντι της πρόβλεψή τους. Η δική μας προσέγγιση συνίστατι στην εκμετάλλευση των διαστημάτων πρόβλεψης που παράγονται από ένα μοντέλο παλινδρόμησης, ώστε να ορίσουμε ένα σύνολο βέλτιστων υπερ-παραμέτρων για κάθε σετ δεδομένων και τελικά να δημιουργήσουμε έναν ensemble με αυτά. Το σύνολο αυτό ορίζεται ως οι υπερ-παράμετροι που βρίσκονται στο p-οστό διάστημα εμπιστοσύνης της πρόβλεψης, όπου το p επιλέχτηκε έτσι ώστε να βελτιστοποιεί την απόδοση του ensemble. Αν η βέλτιστη τιμή βρίσκεται μέσα σε αυτό το διάστημα τότε με χρήση του ensemble θεωρητικά θα εξασφαλιστεί αποτέλεσμα ισάξιο με ένα μοντέλο που θα προέβλεπε επακριβώς τη βέλτιστη τιμή. Μια σύντομη ανάλυση των τρόπων εξαγωγής διαστημάτων πρόβλεψης από μοντέλα παλινδρόμησης βρίσκεται στο Παράρτημα \ref{appendix:Intervals}.
 
\subsection{Ensemble με προς τα εμπρός επιλογή μοντέλων} Η τεχνική της χρήσης διαστημάτων πρόβλεψης από το HPP μοντέλο σε συνδυασμό με τη χρήση διαφορετικών αλγορίθμων μηχανικής μάθησης από το σύστημα δημιουργεί ένα πολυπληθές σύνολο μοντέλων προς εκμετάλλευση. Δεδομένης της ετερογένειας και της εφαρμογής βελτιστοποίησης για κάθε αλγόριθμο θεωρούμε πως το σύνολο μοντέλων που δημιουργούμε πληρεί τις προϋποθέσεις που είχε ορίσει ο \citet{Dietterich2000}:  

\begin{displayquote}
	Μία αναγκαία και ικανή συνθήκη για να είναι μία συλλογή μοντέλων πιο ακριβής από τα μοντέλα που την απαρτίζουν είναι αυτά να είναι ακριβή και ετερογενή. 
\end{displayquote}

Το σύστημά μας βασίζεται στη δουλειά των \citet{Caruana:2004:ESL:1015330.1015432}, οι οποίοι παρουσιάζουν τη μέθοδο του σχηματισμού μίας συλλογής μοντέλων με τη τεχνική της προς τα εμπρός επιλογής μοντέλων. Η τεχνική αυτή, η οποία θυμίζει την προς τα εμπρός επιλογή χαρακτηριστικών που περιγράψαμε στην ενότητα \ref{section:selection} επιλέγει επαναληπτικά να προσθέσει στη συλλογή το μοντέλο που μεγιστοποιεί την απόδοσή της και το ενσωματώνει παίρνοντας το μέσο όρο των προβλέψεων της συλλογής μετά την προσθήκη του. Στο σχήμα \ref{fig: flowHPP} μπορούμε να παρατηρήσουμε τον τρόπο με τον οποίο λειτουργεί η προτεινόμενη μέθοδος.

\begin{figure}[!htb]
	\begin{tikzpicture}[node distance = 3cm]
	% Place nodes
	\node [block_flow] (gather) {\makecell{Συγκέντρωση \\μοντέλων}};
	\node [block_flow, right of=gather] (init) {\makecell{Αρχικοποίηση\\ συλλογής\\ με Ν μοντέλα}};
	\node [block_flow, right of=init] (sample) {\makecell{Δειγματοληψία \\ με\\ αντικατάσταση}};
	\node [block_flow, align = center, right of=sample, xshift = -0.2cm] (eval) {\makecell{Αξιολόγηση\\μοντέλων\\δείγματος}};
	\node [block_flow, align = center, right of=eval, xshift = -0.5cm] (add) {\makecell{Προσθήκη \\ καλύτερου\\ μοντέλου}};
	\node [decision, right of=add, scale = 0.8, xshift = -1.2cm] (decide) {OK?};
	% Draw edges
	 \path [line] (gather) -- (init);
	 \path [line] (init) -- (sample);
	 \path [line] (sample) -- (eval);
	 \path [line] (eval) -- (add);
	 \path [line] (add) -- (decide);
    \path[line] (decide.south)  node [anchor=west] {OXI} |- ([xshift=-0.5cm,yshift=-0.5cm]add.south west) -|  
     ($(init)!.5!(sample)$); 
	\end{tikzpicture}
	\caption[Διάγραμμα ροής της διαδικασίας σχηματισμού μίας συλλογής μοντέλων με την τεχνική της προς τα εμπρός επιλογής μοντέλων]{Διάγραμμα ροής της διαδικασίας σχηματισμού μίας συλλογής μοντέλων με την τεχνική της προς τα εμπρός επιλογής μοντέλων: Η συλλογή αρχικοποιείται με τα Ν καλύτερα μοντέλα και στη συνέχεια εφαρμόζεται bootstrapping, σε κάθε επανάληψη του οποίου επιλέγεται ένα υποσέτ των μοντέλων για αξιολόγηση και προστίθεται το βέλτιστο στη συλλογή. Η συνθήκη τερματισμού αντιστοιχεί στο σχηματισμό ενός ensemble προκαθορισμένου πλήθους μοντέλων ή στην ικανοποίηση κάποιας άλλης ποιοτικής συνθήκης (π.χ. ακρίβεια ensemble).}
	\label{fig: flowHPP}	
\end{figure}

Κατά το σχηματισμό της συλλογής ακολουθούνται κάποιες τεχνικές που στοχεύουν στην αποδοτικότερη σχεδίαση και την αποφυγή υπερ-προσαρμογής:
\begin{itemize}
	\item Επιλογή μοντέλων με αντικατάσταση. Στην περίπτωση που κάθε μοντέλο επιτρέπεται να χρησιμοποιηθεί μόνο μία φορά παρατηρήθηκε το πρόβλημα της απότομης πτώσης της απόδοσης της συλλογής, λόγω της αναγκαστικής συμπερίληψης των εναπομεινάντων "κακών" μοντέλων. 
	\item Αρχικοποίηση συλλογής με τα καλύτερα μοντέλα. Έτσι, αποφεύγεται η υπερ-προσαρμογή στην περίπτωση που διαθέτουμε λίγα μοντέλα.
	\item Εφαρμογή συνάθροισης (bootstrapping) κατά τη δειγματοληψία μοντέλων. Σε κάθε επανάληψη της συνάθροισης επιλέγεται ένα δείγμα από τα διαθέσιμα μοντέλα με πιθανότητα συμπερίληψης ενός μοντέλου $p=0.5$, το οποίο αξιολογείται για την επιλογή του βέλτιστου. Έτσι, αποφεύγεται η υπερ-προσαρμογή στην περίπτωση που διαθέτουμε πολλά μοντέλα, καθώς μειώνεται η πιθανότητα να επιλέξουμε το συνδυασμό μοντέλων που οδηγούν σε αυτή.
\end{itemize} 
\subsection{Ευριστικές}
Συχνά στην πορεία ενός πειράματος μηχανικής μάθησης οι αναλυτές δεδομένων καταφεύγουν στη χρήση ευριστικών. Ως ευριστική ορίζουμε την προσέγγιση της λύσης σε ένα πρόβλημα μέσω μίας πρακτικής μεθόδου, η οποία δεν εγγυάται τη θεωρητικά βέλτιστη λύση, αλλά είναι επαρκώς καλή για το δεδομένο πρόβλημα. Σημαντικές σχεδιαστικές επιλογές βασίζονται, συνειδητά ή ασυνείδητα, σε τέτοιες μεθόδους, που έχουν δοκιμαστεί στο χρόνο και φαίνεται να έχουν ενσωματωθεί στη θεωρία της μηχανικής μάθησης.

Το υπό σχεδίαση σύστημα δε στερείται αυτής της γνώσης, η οποία έχει ενσωματωθεί στον κώδικά του με τη μορφή παραμέτρων στις αποφάσεις που λαμβάνονται στην πορεία ενός πειράματος. Στη συνέχεια παραθέτουμε μερικά παραδείγματα ευριστικών, τα οποία συλλέξαμε από τη βιβλιογραφία:

\paragraph{Το ξυράφι του Όκαμ} Η αρχή αυτή αποδίδεται στον William of Ockham και, ως όρος, εισήχθη από τον Libert Froidmont ~\footnote{https://en.wikipedia.org/wiki/Occam's\_razor}. Συμβουλεύει προς την επιλογή της απλούστερης υπόθεσης μεταξύ ισάξιων, ανταγονιζόμενων υποθέσεων. Στον τομέα της μηχανικής μάθησης χρησιμοποιείται κατά το σχηματισμό του μοντέλου, δίνοντας προτεραιότητα σε απλούστερους αλγορίθμους και απλούστερες παραμετροποιήσεις αλγορίθμων και αποτελεί ευριστική λύση στο πρόβλημα της υπερ-προσαρμογής. 

\paragraph{Κανόνας επάρκειας παραδειγμάτων} Ο Yaser S. Abu-Mostafa  στις διαλέξεις του ~\footnote{http://work.caltech.edu/telecourse.html} παρουσιάζει την εξής ευριστική: προκειμένου να είναι εφικτή η εκπαίδευση ενός αλγορίθμου μηχανικής μάθησης πρέπει να ικανοποιείται η σχέση: 
\begin{equation}
N \geq  10 * d_{vc}
\end{equation}
όπου $N$ είναι το πλήθος των παραδειγμάτων και $d_{vc}$ ο βαθμός του VC-dimension του αλγορίθμου, ο οποίος ορίζεται ως το μέγιστο πλήθος των σημείων που μπορεί να διαχωρίσει το σετ υπόθεσης $H$ του αλγορίθμου και ισούται με:
\begin{itemize}
	\item $d+1$, όπου $d$ η διάσταση της εισόδου, για τους perceptrons.
	\item $d+1$, όπου $d$ το πλήθος των βαρών, για ένα \gls{ΤΝΔ}.
	\item $d$, το πλήθος των διανυσμάτων στήριξης, για ένα \gls{SVM}.
\end{itemize}  


\paragraph{Κανόνας για επιλογή μεγέθους τεστ ελέγχου} Ο ίδιος καθηγητής παρουσιάζει και την ακόλουθη ευριστική. Κατά το διαχωρισμό του σετ δεδομένων σε υποσέτ για εκπαίδευση και έλεγχο πρέπει να επέλθει συμβιβασμός, καθώς είναι σημαντική τόσο η καλή εκτίμηση της απόδοσης όσο και η αντιπροσωπευτικότητά της για το τελικό μοντέλο, ανάγκες που σπρώχνουν προς την αύξηση και των δύο υποσέτ. Ευριστικό κανόνα αποτελεί η επιλογή 
\begin{equation}
k =  \frac{N}{5}
\end{equation}
όπου $k$ το πλήθος των παραδειγμάτων στο σετ ελέγχου και $N$ το συνολικό πλήθος παραδειγμάτων. Η κοινότητα βέβαια συμφωνεί πως η καλύτερη τεχνική είναι αυτή του 10-fold cross-validation (Ενότητα \ref{section:eval}). 

\paragraph{Διατηρούμενη διακύμανση \gls{PCA}} Από μία σειρά διαλέξεων ~\footnote{https://www.coursera.org/learn/machine-learning} προέρχεται και η επόμενη ευριστική: κατά την εφαρμογή PCΑ επιλέγουμε να κρατήσουμε το πλήθος των κυρίαρχων συνιστωσών που εξασφαλίζουν τη διατήρηση του $98\%$ της διακύμανσης των αρχικών χαρακτηριστικών.

\paragraph{Κανόνας Tukey για αναγνώριση ακραίων τιμών} Η αναγνώριση των ακραίων τιμών σε ένα δείγμα γίνεται συνήθως οπτικά, καθώς όπως δήλωσε ο \citet{Grubbs} ως ακραία τιμή ορίζεται αυτή που απέχει πολύ από τις υπόλοιπες. Ο \citet{Tukey} ποσοτικοποίησε το γενικό ορισμό, ορίζοντας άνω και κάτω όρια, πέρα από τα οποία οι τιμές θεωρούνται ακραίες

\begin{equation}
min =  Q_1 - (IQR * 1.5)
max =  Q_3 + (IQR * 1.5)
\end{equation}

όπου $Q_1$ και $Q_3$ το πρώτο και τρίτο τεταρτημόριο και $IQR$ το διατεταρτημοριακό εύρος (interquartile range) της κατανομής ενός δείγματος. 
 

\section{Αρχιτεκτονική}
Το σύστημα Automated Data Scientist αποτελείται από δύο ευκρινώς διαχωριζόμενα υποσυστήματα:
\begin{itemize}
	\item Υποσύστημα εκπαίδευσης. Η εκπαίδευση του συστήματος είναι απαραίτητη προκειμένου να έχει την ικανότητα πρόβλεψης των υπερ-παραμέτρων των αλγορίθμων μάθησης που χρησιμοποιεί o ensemble. Μέσω αυτού του υποσυστήματος είναι δυνατή η παραγωγή των \gls{HPP} μοντέλων, η οποία απαιτεί την εξαγωγή μετα-χαρακτηριστικών, την εφαρμογή αλγορίθμων βελτιστοποίησης και τέλος, την εκπαίδευση των \gls{HPP} μοντέλων. 
	\item Υποσύστημα πειράματος. Περιέχει τη βασική λειτουργικότητα του συστήματος προς τον αναλυτή δεδομένων. Τα συστατικά του αναλαμβάνουν την παραγωγή ενός βέλτιστου ensemble μοντέλων για το σετ δεδομένων εισόδου παρέχοντας τεχνικές προ-επεξεργασίας, οπτικοποίησης,  βελτιστοποίησης υπερ-παραμέτρων αλγορίθμων μηχανικής μάθησης, εκπαίδευσης μοντέλων, σχηματισμού ensemble και αξιολόγησης μοντέλου.
\end{itemize} 
\subsection{Υποσύστημα εκπαίδευσης}
\begin{figure}[!htb]
		\begin{tikzpicture}	[database/.style={
			cylinder,
			cylinder uses custom fill,
			cylinder body fill=white!50,
			cylinder end fill=white!50,
			shape border rotate=90,
			aspect=0.5,
			draw
		}]
		\node[database] (db) at (0, -0.25cm) {Αποθήκη};
		\node[block, align = center, text width=2.5cm, text centered, minimum width = 2 cm] (A) at (2.5cm,1.5) {optimizer};
		\node[block, align = center, text width=2.5cm, text centered, minimum width = 2 cm] (B) at (2.5cm,-1.5) {mfExtractor};	
		\node[database, aspect = 0.2] (mfdb) at (6cm,-0cm) {\makecell{Αποθήκη \\μετα-δεδομένων}};	
		\node[block, align = center, text width=2.5cm, text centered, minimum width = 2 cm] (C) at (9.5cm, -0.6cm) {metaLearner};
		\node[database] (model)  at (13cm, 0cm) {HPP-Μοντέλο};	
		\draw[->, thick] (db) |- (A);
		\draw[->, thick] (db) |- (B);
		\draw[->, thick] (A) -| (mfdb.north);
		\draw[->, thick] (B) -| (mfdb.south);
		\draw[->, thick] (mfdb) to (C);
		\draw[->, thick] (C) to (model.west);
		\end{tikzpicture}
		\caption[Το υποσύστημα εκπαίδευσης]{Το υποσύστημα εκπαίδευσης: Για κάθε σετ δεδομένων δυαδικής ταξινόμησης της Αποθήκης γίνεται εξαγωγή των μετα-χαρακτηριστικών και εύρεση των βέλτιστων υπερ-παραμέτρων με αποτέλεσμα τη δημιουργία ενός σετ-μεταδεδομένων για κάθε υπερ-παράμετρο ενός αλγορίθμου μάθησης. Το πακέτο metaLearner αναλαμβάνει την εκπαίδευση των HPP μοντέλων.}
\end{figure}

Συστατικά αυτού του υποσυστήματος αποτελούν:
\begin{itemize}
	\item Αποθήκη. Πρόκειται για το σύνολο σετ δεδομένων δυαδικής ταξινόμησης, τα οποία έχουν συλλεχθεί για την εκπαίδευση των HPP μοντέλων.
	\item optimizer. Το πακέτο αυτό αναλαμβάνει τη βελτιστοποίηση των υπερ-παραμέτρων για δεδομένο αλγόριθμο μάθησης και σετ δεδομένων. Για να το πετύχει αυτό διαθέτει διεπαφή προς την εξωτερική βιβλιοθήκη HPOlib.
	\item mfExtractor. Πακέτο υπεύθυνο για την εξαγωγή μετα-χαρακτηριστικών για ένα σετ δεδομένων.
	\item Αποθήκη μετα-δεδομένων. Πρόκειται για μια συλλογή σετ δεδομένων στα οποίο κάθε παράδειγμα έχει ως χαρακτηριστικά τα μετα-χαρακτηριστικά και κλάση τη βελτιστοποιημένη υπερ-παράμετρο για ένα σετ δεδομένων και συγκεκριμένο αλγόριθμο.
	\item metaLearner. Το πακέτο αυτό αναλαμβάνει την εκπαίδευση ενός HPP μοντέλου για κάθε υπερ-παράμετρο ενός αλγορίθμου μάθησης.
	\item HPP μοντέλο. Τελική έξοδος του υποσυστήματος αποτελεί το HPP-Μοντέλο, το οποίο θα χρησιμοποιηθεί από το υποσύστημα πειράματος. Εκτός από το μοντέλο παρέχεται και πληροφορία χρήσιμη για το καθορισμό των διαστημάτων πρόβλεψης κατά τη πρόβλεψη των υπερ-παραμέτρων.
\end{itemize}


\subsection{Υποσύστημα πειράματος}
\begin{figure}[!htb]
	\begin{tikzpicture}	[database/.style={
		cylinder,
		cylinder uses custom fill,
		cylinder body fill=white!50,
		cylinder end fill=white!50,
		shape border rotate=90,
		aspect=0.5,
		draw
	}]
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (A) at (0,0) {preprocessor};
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (B) at ([xshift = 7em, yshift = 5em] A) {mfExtractor};
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (C) at ([xshift = 7em, yshift = 0em] B) {optimizer};
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (D) at ([xshift = 7em, yshift = 0em] C) {classifier1};
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (E) at ([xshift = 0em, yshift = -5em] D) {classifier2};
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (F) at ([xshift = 0em, yshift = -5em] E) {classifier3};
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (G) at ([xshift = 7em, yshift = 0em] E) {ensembler};
	\node[block, align = center, text width=2cm, text centered, minimum width = 2 cm] (H) at ([xshift = 7em, yshift = 0em] G) {evaluator};
	\draw[->, thick] (A) |- (B);
	\draw[->, thick] (B) to (C);
	\draw[->, thick] (C) to (D);
	\draw[->, thick] (E) to (G);
	\draw[->, thick] (G) to (H);
	\draw[->, thick] (A) to (E);
	\draw[->, thick] (A) -| ([xshift=-0.3cm]E.west) |- (D.west) ;
	\draw[->, thick] (A) -| ([xshift=-0.3cm]E.west) |- (F.west) ;
	\end{tikzpicture}
	\caption[Το υποσύστημα πειράματος]{Το υποσύστημα πειράματος: Δεδομένου ενός σετ δεδομένων δυαδικής ταξινόμησης στην είσοδο το υποσύστημα αυτό εφαρμόζει την κατάλληλη προ-επεξεργασία και στη συνέχεια εξάγει τα μετα-χαρακτηριστικά, ώστε το πακέτο optimizer να προβλέψει τις βέλτιστες υπερ-παραμέτρους με τη βοήθεια των HPP μοντέλων. Στη συνέχεια εκπαιδεύεται ένα πλήθος μοντέλων για κάθε αλγόριθμο μάθησης και το πακέτο ensembler αναλαμβάνει το σχηματισμό του τελικού ensemble. Τελευταίο στάδιο αποτελεί η αξιολόγηση του πειράματος. }
\end{figure}

Τα πακέτα που υλοποιούν τη πλήρη διαδικασία της μηχανικής μάθησης για ένα σετ δεδομένων είναι:
\begin{itemize}
	\item preprocessor. Περιέχει τεχνικές προ-επεξεργασίας όπως καθαρισμού δεδομένων (αντιμετώπιση άγνωστων και άπειρων τιμών), κανονικοποίησης (z-score και min-max), μετασχηματισμού χαρακτηριστικών (PCA, μετασχηματισμός Box-Cox).
	\item mfExtractor. Πρόκειται για το ίδιο πακέτο με αυτό που περιγράφηκε στο υποσύστημα εκπαίδευσης.
	\item optimizer. Στην προκειμένη το πακέτο αυτό αναλαμβάνει την πρόβλεψη των υπερ-παραμέτρων κάθε αλγορίθμου μάθησης χρησιμοποιώντας τα ήδη εκπαιδευμένα HPP μοντέλα.    
	\item classifier$_i$. Κάθε classifier αντιστοιχεί σε ένα μοντέλο με μοναδικό συνδυασμό υπερ-παραμέτρων και αλγορίθμου μάθησης.
	\item ensembler. Το πακέτο αυτό σχηματίζει τον τελικό ensemble από τα διαθέσιμα μοντέλα με την τεχνική του model selection.
	\item evaluator. Υπεύθυνο για την αξιολόγηση του τελικού ensemble με χρήση τεχνικών που είδαμε στην ενότητα \ref{section:eval} και τη σύγκριση της μεθόδου μας με άλλες μεθόδους αναφοράς, οι οποίες αναλύονται στην ενότητα \ref{section:eval_system} με χρήση στατιστικών τεστ.
\end{itemize}