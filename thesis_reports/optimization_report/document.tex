\documentclass[]{article}

\usepackage[autostyle]{csquotes}

\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[
    backend=biber,
    citestyle=numeric,
    style = numeric,
    sortlocale=de_DE,
    natbib=true,
    url=false, 
    doi=true,
    eprint=false
]{biblatex}
\appto{\bibsetup}{\raggedright}
\addbibresource{bibliography.bib}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	citecolor = darkgray,
	linkcolor = black,
	urlcolor = black
}
\usepackage[T1]{fontenc}
\usepackage{kpfonts}%  for math 
\usepackage{xgreek}
\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{Linux Libertine O}
\usepackage{amsmath,amsfonts,amsthm, amssymb} % Math packages
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{graphicx}	
%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}

%%% Custom sectioning

\usepackage{sectsty}

\allsectionsfont{\centering \normalfont\scshape}

%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
	%\vspace{-1in} 	
	\usefont{OT1}{bch}{b}{n}
	\normalfont \normalsize \textsc{} \\ 
	\horrule{0.5pt} \\[0.4cm]
	\huge Βελτιστοποίηση υπερπαραμέτρων \\αλγορίθμων μηχανικής μάθησης \\
	\horrule{2pt} \\[0.5cm]
}
\author{
	\normalfont 								\normalsize
	Νησιώτη Ελένη\\[-3pt]		\normalsize
	\today
}
\date{}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}%
%% ##############################
\begin{document}
	\maketitle
	\section{Περιγραφή του προβλήματος }
	Ένα βασικό στάδιο κατά την εκπαίδευση αλγορίθμων μηχανικής μάθησης είναι αυτό της επιλογής των υπερπαραμέτρων του μοντέλου.
	
	Μαθηματικά το πρόβλημα μπορεί να διατυπωθεί ως εξής: σκοπός ενός πειράματος μηχανικής μάθησης είναι η εκπαίδευση ενός μοντέλου $M$, το οποίο ελαχιστοποιεί μία προκαθορισμένη συνάρτηση κόστους $L(X^{(te)};M)$ σε ένα δεδομένο σετ δεδομένων. Το μοντέλο $Μ$ κατασκευάζεται από έναν αλγόριθμο μάθησης $Α$, ο οποίος παραμετροποιείται από ένα σύνολο παραμέτρων $\lambda$.
	
	Καταληγούμε λοιπόν στον μαθηματικό ορισμό της εύρεσης του συνόλου των υπερπαραμέτρων $\lambda^*$, που ορίζουν το βέλτιστο μοντέλο $M^*$
	
	
	\begin{equation}
	\label{opt}
		\lambda^* = \argmin{L(X^{(te)}; A(X^{(tr)}; \lambda))} = \argmin_{\lambda}{L(\lambda; A, X^{(tr)}, L)}
	\end{equation} 
	όπου $X^{(tr)}$ το σετ δεδομένων και $X^{(te)}$ το σετ ελέγχου
	\section{Τεχνικές βελτιστοποίησης}
	Μερικά χαρακτηριστικά της παραπάνω συνάρτησης είναι τα εξής:
	\begin{itemize}
		\item είναι μια συνάρτηση μαύρου κουτιού, δηλαδή περιγράφεται μόνο μέσω εισόδων-εξόδων.
		\item δεν έχουμε γνώση για τις παραγώγους της, το οποίο είναι άμεσο επακόλουθο της προηγούμενης πρότασης.
		\item είναι non-convex. 
		\item δεν εξαρτάται εξίσου από όλες τις παραμέτρους. 
		\item ο υπολογισμός της για δεδομένο $\lambda$ είναι υπολογιστικά και χρονικά απαιτητικός.
	\end{itemize}
	
	Η θεωρία της βελτιστοποίησης συναρτήσεων έχει προσφέρει ποικίλλες επιλογές στην επίλυση του υπό μελέτη προβλήματος. Eξελικτικοί αλγόριθμοι \citep{1554741}, gradient decent \citep{wassenberg},  αλγόριθμοι \citep{Huang2006} βασισμένοι σε ευριστικές \citep{Nelder01011965}. Τα χαρακτηριστικά ωστόσο που αναφέρουμε προσδίδουν στη βελτιστοποίηση \ref{opt} ιδιαιτερότητες, που συγκεκριμενοποιούν τον κατάλληλο αλγόριθμο βελτιστοποίησης. 
	\subsection{Bayesian βελτιστοποίηση}
	Η τεχνική αυτή επικεντρώνεται στην απαίτηση βελτιστοποίησης μιας άγνωστης, κοστοβόρας συνάρτησης με μικρό πλήθος evaluations. Προς αυτό το σκοπό αντικαθιστά την $L$ με ένα πιθανοτικό μοντέλο, το οποίο ενσωματώνει πληροφορία για όλα τα προηγούμενα γνωστά evaluated σημεία, και στη συνέχεια το εκμεταλλεύεται για να αποφασίσει το επόμενο σημείο αξιολόγησης (evaluation).
	
	Δύο είναι οι βασικές σχεδιαστικές επιλογές κατά την εφαρμογή αυτής της μεθόδου: 
	
	\paragraph{prior over functions} περιέχει υποθέσεις σχετικά με την άγνωστη συνάρτηση $L$
	
	Μία συνήθης επιλογή \citep{practical} αποτελεί η χρήση Γκαουσιανών διαδικασιών ως priors. Πρόκειται για ένα σύνολο τυχαίων μεταβλητών, για το οποίο ισχύει ότι οποιοδήποτε υποσύνολο έχει γκαουσιανή κατανομή πολλών μεταβλητών. 
	
	Η τυχαία μεταβλητή ορίζεται ως εξής:
	\begin{equation}
		X=
		\begin{cases}
			0 & \textit{case 1}\\
			1 & \textit{case 2}
		\end{cases}
	\end{equation} 
	
	Η γκαουσιανή κατανομή ορίζεται ως εξής
	\begin{equation}
		X \approx N (\mu, \sigma^2) = \frac{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}
	\end{equation}
	
	Η γκαουσιανή κατανομή πολλών μεταβλητών χαρακτηρίζεται από το γεγονός ότι ο γραμμικός συνδυασμός τους δίνει γκαουσιανή κατανομή. Χρησιμοποιείται για την περιγραφή συσχετισμένων τυχαίων μεταβλητών.
	
	\paragraph{Συνάρτηση Απόκτησης} χρησιμοποιείται για την απόδοση "χρησιμότητας εξερεύνησης" στο χώρο με βάση το πιθανοτικό μοντέλo. Οι συναρτήσεις που χρησιμοποιούνται στη βιβλιογραφία είναι:
	\begin{itemize}
		\item Πιθανότητα βελτίωσης
		\item Προσδοκώμενη βελτίωση
		\item Άνω όρια εμπιστοσύνης
	\end{itemize}
	
	Η προσδοκώμενη βελτίωση χρησιμοποιείται σχεδόν καθολικά, καθώς δίνει καλύτερα πειραματικά αποτελέσματα, είναι κατανοητή (intuitive) και δε χρειάζεται ρύθμιση \citep{practical},  \citep{Bergstra:2011:AHO:2986459.2986743}.
	
	
	Η προσέγιση της $L$ που εμφανίζεται στην \ref{opt} μπορεί να γίνει με τη χρήση μοντέλου, οπότε η βελτιστοποίηση ονομάζεται Sequential Model-Based Optimization (SMBO). Παραδείγμετα χωρίς χρήση μοντέλου είναι οι αλγόριθμοι Random Online Adaptive Racing (ROAR) \citep{Hutter2011}, Covari\-ance Matrix Adaptation Evolution Strategy (CMA-ES) \citep{DBLP:journals/corr/LoshchilovH16}, Sequential Model-free Optimization (SMFO) \citep{7373431}
	
	\subsubsection{SMBO} 

	Η υπεροχή των SMBO έγκειται στη δυνατότητα παρεμβολής (interpolation) μεταξύ παρατηρούμενων σετ υπερπαραμέτρων και παρέκτασης (extrapolation) σε άγνωστες περιοχές του χώρου παραμέτρων. Επίσης, ποσοτικοποιούν τη σημασία κάθε υπερπαραμέτρου και των αλληλεξαρτήσεων. 
	
	Μερικά προβλήματα, τα οποία αντιμετωπίζει η κοινωνία του machine learning οφείλονται σε  χαρακτηριστικά των SMBO αλγοριθμών, που προκύπτουν από την ικανότητά τους να λύνουν γενικής φύσης προβλήματα βελτιστοποίησης συναρτήσεων μαύρου κουτιού, εισάγοντας περιορισμούς ανυπόστατους στη μηχανική μάθηση. Τέτοιοι περιορισμοί είναι:
	\begin{itemize}
		\item υπόθεση ντετερμινιστικότητας της συνάρτησης προς βελτιστοποίηση
		\item ακριβή (costly) σχεδίαση των αρχικών πειραμάτων
		\item εξάρτηση από υπολογιστικά ακριβά μποντέλα
		\item υπόθεση ίδιου κόστος για κάθε υπολογισμό (run) του αλγορίθμου-στόχου  
	\end{itemize}
	
	Παραδείγματα SMBO αλγορίθμων είναι:
	\begin{itemize}
		\item Sequential Model-based Algorithm Configuration (SMAC) \citep{Hutter2011}
		\item Tree Parzen Estimator \citep{Bergstra:2011:AHO:2986459.2986743}
		\item Spearmint, ο οποίος αναλύεται από τους \citep{practical}, χωρίς ωστόσο να αναφέρεται το όνομά του.
	\end{itemize}
	
	\section{Ειδικά Ζητήματα}
	\subsection{Χώρος υπερπαραμέτρων}
	Το πλήθος των μεταβλητών που αποτελούν άξονες του χώρου διαφέρει αισθητά μεταξύ διαφορετικών αλγορίθμων μηχανικής μάθησης, ενώ το είδος περιλαμβάνει συνεχείς, ακέραιες και κατηγορικές μεταβλητές. Ένα βασικό χαρακτηριστικό του χώρου είναι η δενδρική μορφή, η οποία υποδηλώνει υποθετική παρουσία μεταβλητών δεδομένων προγόνων τους, για παράδειγμα το πλήθος των νευρώνων στο δεύτερο επίπεδο ενός νευρωνικού λαμβάνει υπόσταση όταν το πλήθος των επιπέδων είναι τουλάχιστον 2.
	\subsection{Συνάρτηση κόστους}
	Η επιλογή της συνάρτησης κόστους περιλαμβάνει τις απαιτήσεις του προβλήματος βελτιστοποίησης και τις παραδοχές που έχουν γίνει. Τεχνικές που συναντώνται στη σύγχρονη βιβλιογραφία είναι:
	\begin{itemize}
		\item ελαχιστοποίηση σφάλματος του μοντέλου 
		\item ελαχιστοποίηση των evaluations, τεχνική η οποία υποθέτει ίδιο χρόνο για κάθε evaluation
		\item ελαχιστοποίηση του χρόνου διαδικασίας βελτιστοποίησης
		\item συνδυασμός μετρικών, όπως το Adjusted Ratios of Ratio (ARR) \citep{Abdulrahman:2014:MCA:3015544.3015557}
		\item οι \citet{HutHooLeyMur10} εισάγουν περιορισμούς και μηχανισμούς στον αλγόριθμο Sequential Parameter Optimization \citep{1554761}, αναγκάζοντας τον αλγόριθμο να λειτουργεί με συγκεκριμένο χρονικό budget. Αυτό έχει ως αποτέλεσμα τη μείωση του χρόνου, χωρίς όμως αυτό να είναι απαραίτητητα το κριτήριο βελτιστοποίησης.
	\end{itemize}

	\subsection{Αξιολόγηση και έλεγχος}
	Στο τέλος της βελτιστοποίησης πρέπει να αξιολογηθούν οι υπερπαράμετροι που επιλέχθηκαν ως προς την επίτευξη του στόχου, αλλά και να συγκριθούν σετ υπερπαραμέτρων που παρήχθησαν με διαφορετικές τεχνικές, ώστε και αυτές να αξιολογηθούν.
	\subsubsection{Αξιολόγηση αλγορίθμου βελτιστοποίσης}
	Ο αλγόριθμος βελτιστοποίησης οφείλει να αξιολογηθεί ως προς την ικανότητά του να αποδίδει στις υπερπαραμέτρους το σωστό κόστος, να βρίσκει το βέλτιστο συνδυασμό υπερπαραμέτρων και ενδεχομένως να ελαχιστοποιεί τους καταναλώσιμους πόρους. Μερικοί βασικοί δείκτες για την αξιολόγηση ενός αλγορίθμου βελτιστοποίησης σύμφωνα με τους \citet{HutHooLeyMur10} είναι οι εξής:
	\begin{itemize}
		\item η ρίζα της μέσης τιμής της διαφοράς μεταξύ προβλεφθείσας και πραγματικής απόδοσης των υπερπαραμέτρων(RMSE)
		\item Quality of predictive ranks, η οποία υπολογίζεται μέσω του συντελεστή συσχέτισης Spearman. Πρόκειται για μία μη-παραμετρική μετρική συσχέτισης κατάταξης, η οποία εκτιμά τη δυνατότητα περιγραφής της σχέσης μεταξύ δύο μεταβλητών μέσω μίας μονότονης συνάρτησης.
		\item EIC quality. Πρόκειται για το συντελεστή συσχέτισης Spearman μεταξύ της πραγματικής απόδοσης και του Expected Improvement Criterion, το οποίο χρησιμοποιείται στην acquisition function με βάση το πιθανοτικό μοντέλο. Ο δείκτης αυτός είναι χρήσιμος για την αξιολόγηση του μοντέλου σε έναν SMBO αλγόριθμο, καθώς η βασική χρήση του μοντέλου είναι στην επιλογή υποσχόμενων υπερπαραμέτρων με βάση το EIC.
		\item χρόνος διεξαγωγής βελτιστοποίησης σε μορφή Box plots του $\log_{10} (time)$
	\end{itemize}
	
	Συχνή είναι και η χρήση διαγραμμάτων:
	
	\paragraph{Generalization performance plots}
	Σύμφωνα με τους \citet{wassenberg} τα διαγράμματα γενίκευσης απόδοσης ως προς το χρόνο είναι καταλληλότερα σε σχέση με τα sub-optimality plots που χρησιμοποιούνται συχνά στην αξιολόγηση βελτιστοποίησης συναρτήσεων.
	
	
	\paragraph{AUC vs generation}
	Μία εναλλακτική των generalization performance plots, στην οποία φαίνεται το εμβαδό της περιοχής κάτω από τη καμπύlη Receiver Operating Characteristic (Area under Curve) σε συνάρτηση των γενεών του αλγορίθμου βελτιστοποίησης χρησιμοποίεται από τους \citet{1554741}
	
	
	
	
	\subsubsection{Σύγκριση μεταξύ διαφορετικών τεχνικών}
	Σε περίπτωση που έχουμε διεξάγει πειράματα για δύο διαφορετικές τεχνικές βελτιστοποίησης, τα οποία έχουν δημιουργήσει δύο σετ λύσεων, που το καθένα περιλαμβάνει πειράματα για τη δεδομένη τεχνική και ποικίλλα σετ δεδομένων και θέλουμε να διαπιστώσουμε στατιστικά σημαντική διαφορά μεταξύ των δύο τεχνικών. 
	
	
	\paragraph{Wilcoxon signed-rank}
	Πρόκειται για ένα ισχυρό στατιστικό τεστ για τη σύγκριση μεθόδων σε διαφορετικά σετ δεδομένων και είναι μία μη-παραμετρική εκδοχή του Student's t-test, η οποία δεν απαιτεί την παραδοχή της κανονικής κατανομής. Xρησιμοποιείται από τους \citet{ensemble}  
	\paragraph{Θηκογράμματα (Box plots)}
	Η χρήση Box plots οπτικοποιεί την κατανομή των λύσεων που έχει δωθεί από μία τεχνική βελτιστοποίησης και η παράθεση διαφορετικών Box plots στο ίδιο διάγραμμα χρησιμοποιείται συχνά \citep{Hutter2011} για σύγκριση μεταξύ τεχνικών. 
	\paragraph{Mann-Whitney U test}
	Το Μann-Whitney U τεστ, ή Wilcoxon rank-sum τεστ, είναι ένα μη παραμετρικό τεστ της μηδενικής υπόθεσης ότι είναι εξίσου πιθανό μία τυχαία επιλεχθείσα τιμή από το ένα σύνολο να είναι μικρότερη ή μεγαλύτερη από μία τυχαία τιμή του δεύτερου συνόλου. Σε αντίθεση με το t-test δεν απαιτεί την υπόθεση κανονικής κατανομής και είναι σχεδόν τόσο αποτελεσματικό όσο το πρώτο σε κανονικές κατανομές. \citep{Hutter2011} \citep{Hutter:2009:EIM:1569901.1569940}
	
	\subsubsection{Εκτίμηση σημασίας υπερπαραμέτρων}
	Η κατανόηση της επιρροής που έχει κάθε υπερπαράμετρος στην απόδοση ενός αλγορίθμου μάθησης, καθώς και η συσχέτιση μεταξύ των υπερπαραμέτρων μπορεί να προσφέρει μία ποιοτική βάση στο πρόβλημα βελτιστοποίησης, αναδεικνύοντας τις σημαντικές υπερπαραμέτρους και γενικότερα υποχώρους του χώρου παραμετροποίησης. 
	
	Οι \citet{HutHooLey14:fanova} κατάφεραν με χρήση της τεχνικής ANOVA και ενός αλγορίθμου υπολογισμού των marginal predictions μίας συνάρτησης μαύρου κουτιού με random forests να αναδείξουν σημαντικές υπερπαραμέτρους και συσχετίσεις μεταξύ αυτών.
	
	\subsection{Επιτάχυνση βελτιστοποίησης}
	Οι χρονικοί και υπολογιστικοί πόροι καθορίζουν την ακρίβεια του αποτελέσματος της βελτιστοποίησης, η οποία σε πολλές περιπτώσεις είναι απαγορευτικά ακριβή. 
	
	Παραδείγματα της προσπάθειας επιτάχυνσης της διαδικασίας είναι:
	\begin{itemize}
		\item η εισαγωγή χρονικών ορίων που είδαμε \citep{HutHooLeyMur10}, 
		\item η προσέγγιση της προβαλλόμενης Διαδικασίας (Projected Process Approxima\-tion) \citep{HutHooLeyMur10}, η οποία μειώνει την πολυπλοκότητα των Γκαουσιανών Διαδικασιών.
		\item η χρήση της τεχνικής Cholesky Decomposition \citep{Schilling2016} κατά τον υπολογισμό του αντίστροφου του πίνακα πυρήνα (που προκύπτει στη ρύθμιση των Γκαουσιανών Διαδικασιών).
	\end{itemize} 
	\section{Χρήση μετα-γνώσης}
    Ως μετα-γνώση ορίζουμε την πληροφορία που έχει παραχθεί σε παλαιότερα πειράματα με διαφορετικά σετ δεδομένων, την οποία θα θέλαμε να ενσωματώσουμε στο σετ δεδομένων υπό βελτιστοποίηση.
    
    Οι \citet{Schilling2016} αναγνωρίζουν δύο άξονες εφαρμογής μετα-γνώσης στο πρόβλημα της βελτιστοποίησης:
    \begin{itemize}
    	\item χρήση μετα-χαρακτηριστικών των παρελθοντικών σετ δεδομένων
    	\item χρήση μετα-γνώσης για την αρχικοποίηση του αλγορίθμου βελτιστοποίησης. \citep{AAAI1510029}
    \end{itemize}
    Οι ίδιοι, κινούμενοι στον πρώτο άξονα, υπολογίζουν την απόδοση 50 διαφορετικών σετ δεδομένων σε ένα πλέγμα υπερπαραμέτρων, καθώς και μετα-χαρακτηριστικά των σετ δεδομένων δημιουργώντας ένα σετ μετα-δεδομένων, το οποίο χρησιμοποιούν κατά τη βελτιστοποίηση. Είναι σημαντικό πως για κάθε σετ δεδομένων χρησιμοποιούν μία διαφορετική γκαουσιανή διαδικασία, συναθροίζοντας  την πληροφορία με χρήση της θεωρίας των γινομένων γκαουσιανών διαδικασιών.
    \nocite{*}
    \printbibliography[title= Βιβλιογραφία]
\end{document}